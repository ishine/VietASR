# Copyright    2021-2023  Xiaomi Corp.        (authors: Fangjun Kuang,
#                                                       Wei Kang,
#                                                       Zengwei Yao)
#
# See ../../../../LICENSE for clarification regarding multiple authors
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import k2
import torch
import torch.nn as nn
import torchaudio
import torchaudio.functional
# from encoder_interface import EncoderInterface
from subsampling import Conv2dSubsampling
from icefall.utils import add_sos, make_pad_mask
from zipformer import Zipformer2
from typing import Dict, List, Tuple, Optional, Union
from scaling import ScheduledFloat



def _to_int_tuple(s: str):
	return tuple(map(int, s.split(",")))


def LayerNorm(normalized_shape, eps=1e-5, elementwise_affine=True, export=False):
  return torch.nn.LayerNorm(normalized_shape, eps, elementwise_affine)


def do_rnnt_pruning(
	am: torch.Tensor, blank: torch.Tensor, lm: torch.Tensor, vocab: torch.Tensor, ranges: torch.Tensor
) -> Tuple[torch.Tensor, torch.Tensor]:
	"""Prune the output of encoder(am) and prediction network(lm) with ranges
	generated by `get_rnnt_prune_ranges`.

	Args:
	  am:
		The encoder output, with shape (B, T, encoder_dim)
	  lm:
		The prediction network output, with shape (B, S + 1, decoder_dim)
	  ranges:
		A tensor containing the symbol indexes for each frame that we want to
		keep. Its shape is (B, T, s_range), see the docs in
		`get_rnnt_prune_ranges` for more details of this tensor.

	Returns:
	  Return the pruned am and lm with shape (B, T, s_range, C)
	"""
	# am (B, T, encoder_dm)
	# lm (B, S + 1, decoder_dim)
	# ranges (B, T, s_range)
	assert ranges.shape[0] == am.shape[0], (ranges.shape, am.shape)
	assert ranges.shape[0] == lm.shape[0], (ranges.shape, lm.shape)
	assert am.shape[1] == ranges.shape[1], (am.shape, ranges.shape)
	(B, T, s_range) = ranges.shape
	(B, S1, blank_dim) = blank.shape
	(B, S1, vocab_dim) = blank.shape
	(B, S1, lm_dim) = lm.shape
	encoder_dim = am.shape[-1]
	assert am.shape == (B, T, encoder_dim), (am.shape, B, T, encoder_dim)

	# (B, T, s_range, encoder_dim)
	am_pruned = am.unsqueeze(2).expand((B, T, s_range, encoder_dim))

	# (B, T, s_range, decoder_dim)
	blank_pruned = torch.gather(
		blank,
		dim=1,
		index=ranges.reshape(B, T * s_range, 1).expand(
			(B, T * s_range, blank_dim)
		),
	).reshape(B, T, s_range, blank_dim)

	lm_pruned = torch.gather(
		lm,
		dim=1,
		index=ranges.reshape(B, T * s_range, 1).expand(
			(B, T * s_range, lm_dim)
		),
	).reshape(B, T, s_range, lm_dim)

	vocab_pruned = torch.gather(
		vocab,
		dim=1,
		index=ranges.reshape(B, T * s_range, 1).expand(
			(B, T * s_range, vocab_dim)
		),
	).reshape(B, T, s_range, vocab_dim)
	return am_pruned, blank_pruned, lm_pruned, vocab_pruned


class ImprovedFactorizedTransducer(nn.Module):
	def __init__(
		self,
		encoder_embed: nn.Module,
		encoder: nn.Module,
		blank_decoder: Optional[nn.Module],
		vocab_decoder: Optional[nn.Module],
		joiner: Optional[nn.Module],
		args,
	):
		"""A joint CTC & Transducer ASR model.

		- Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks (http://imagine.enpc.fr/~obozinsg/teaching/mva_gm/papers/ctc.pdf)
		- Sequence Transduction with Recurrent Neural Networks (https://arxiv.org/pdf/1211.3711.pdf)
		- Pruned RNN-T for fast, memory-efficient ASR training (https://arxiv.org/pdf/2206.13236.pdf)

		Args:
		  encoder_embed:
			It is a Convolutional 2D subsampling module. It converts
			an input of shape (N, T, idim) to an output of of shape
			(N, T', odim), where T' = (T-3)//2-2 = (T-7)//2.
		  encoder:
			It is the transcription network in the paper. Its accepts
			two inputs: `x` of (N, T, encoder_dim) and `x_lens` of shape (N,).
			It returns two tensors: `logits` of shape (N, T, encoder_dim) and
			`logit_lens` of shape (N,).
		  decoder:
			It is the prediction network in the paper. Its input shape
			is (N, U) and its output shape is (N, U, decoder_dim).
			It should contain one attribute: `blank_id`.
			It is used when use_transducer is True.
		  joiner:
			It has two inputs with shapes: (N, T, encoder_dim) and (N, U, decoder_dim).
			Its output shape is (N, T, U, vocab_size). Note that its output contains
			unnormalized probs, i.e., not processed by log-softmax.
			It is used when use_transducer is True.
		  use_transducer:
			Whether use transducer head. Default: True.
		  use_ctc:
			Whether use CTC head. Default: False.
		"""
		super().__init__()
		# assert isinstance(encoder, EncoderInterface)
		self.encoder_embed = encoder_embed
		self.encoder = encoder
		self.blank_decoder = blank_decoder		# blank decoder and vocab decoder are basically the same structure
		self.vocab_decoder = vocab_decoder
		self.joiner = joiner
		self.train_stage = args.train_stage
		self.use_local_rnnt_loss = args.use_local_rnnt_loss

		"""
		Blank decoder and vocab decoder are basically the same structure,
		output with same decoder dim, the differences lies in the joiner
		With embedding layer, num_embeddings equal the vocab_size (with <blank> token)
		however, the vocab does not include a <pad> token, and uses <blk> as padding_idx
		"""

	@classmethod
	def build_encoder_embed(self, params) -> nn.Module:
		"""
		encoder_embed converts the input of shape (N, T, num_features)
		to the shape (N, (T - 7) // 2, encoder_dims).
		That is, it does two things simultaneously:
		  (1) subsampling: T -> (T - 7) // 2
		  (2) embedding: num_features -> encoder_dims
		In the normal configuration, we will downsample once more at the end
		by a factor of 2, and most of the encoder stacks will run at a lower
		sampling rate.
		"""
		
		encoder_embed = Conv2dSubsampling(
			in_channels=params.feature_dim,
			out_channels=_to_int_tuple(params.encoder_dim)[0],
			dropout=ScheduledFloat((0.0, 0.3), (20000.0, 0.1)),
		)
		if params.pretrain_path is not None:
			checkpoint = torch.load(params.pretrain_path, map_location=torch.device("cpu"))
			checkpoint = checkpoint['model']
			new_checkpoint = OrderedDict()
			prefix = "encoder_embed."
			for item in checkpoint:
				if item.startswith(prefix):
					new_checkpoint[item[len(prefix):]] = checkpoint[item]
			encoder_embed.load_state_dict(new_checkpoint)

		return encoder_embed


	@classmethod
	def build_encoder(self, params) -> nn.Module:
		encoder = Zipformer2(
			output_downsampling_factor=2,
			downsampling_factor=_to_int_tuple(params.downsampling_factor),
			num_encoder_layers=_to_int_tuple(params.num_encoder_layers),
			encoder_dim=_to_int_tuple(params.encoder_dim),
			encoder_unmasked_dim=_to_int_tuple(params.encoder_unmasked_dim),
			query_head_dim=_to_int_tuple(params.query_head_dim),
			pos_head_dim=_to_int_tuple(params.pos_head_dim),
			value_head_dim=_to_int_tuple(params.value_head_dim),
			pos_dim=params.pos_dim,
			num_heads=_to_int_tuple(params.num_heads),
			feedforward_dim=_to_int_tuple(params.feedforward_dim),
			cnn_module_kernel=_to_int_tuple(params.cnn_module_kernel),
			dropout=ScheduledFloat((0.0, 0.3), (20000.0, 0.1)),
			warmup_batches=4000.0,
			causal=params.causal,
			chunk_size=_to_int_tuple(params.chunk_size),
			left_context_frames=_to_int_tuple(params.left_context_frames),
		)
		if params.pretrain_path is not None:
			checkpoint = torch.load(params.pretrain_path, map_location=torch.device("cpu"))
			checkpoint = checkpoint['model']
			new_checkpoint = OrderedDict()
			prefix = "encoder."
			for item in checkpoint:
				if params.pretrain_type=="SSL":
					if item == "encoder.downsample_output.bias":
						continue
				if item.startswith(prefix):
					new_checkpoint[item[len(prefix):]] = checkpoint[item]
			missing_keys, unexpected_keys = encoder.load_state_dict(new_checkpoint, strict=False)
			print(f"missing_keys: {missing_keys}, unexpected_keys: {unexpected_keys}")
		return encoder


	@classmethod
	def build_decoder(self, params) -> nn.Module:
		decoder = IFNTDecoder(
			vocab_size=params.vocab_size,
			embedding_dim=params.decoder_embedding_dim,
			blank_id=params.blank_id,
			num_layers=params.num_decoder_layers,
			hidden_dim=params.decoder_dim,
			output_dim=-1,
		)
		return decoder


	@classmethod
	def build_joiner(self, params) -> nn.Module:
		joiner = IFNTJoiner(
			joint_dim=params.joiner_dim,
			encoder_hidden_dim=max(_to_int_tuple(params.encoder_dim)),
			decoder_hidden_dim=params.decoder_dim,
			vocab_size=params.vocab_size,
			blank_id=params.blank_id
		)
		return joiner


	@classmethod
	def build_model(self, params) -> nn.Module:
		assert params.use_transducer or params.use_ctc, (
			f"At least one of them should be True, "
			f"but got params.use_transducer={params.use_transducer}, "
			f"params.use_ctc={params.use_ctc}"
		)
		assert params.model_type == "IFNT", (
			f"Invalid model type: {params.model_type}, "
			f"model type should only be IFNT"
		)

		encoder_embed = self.build_encoder_embed(params)
		encoder = self.build_encoder(params)

		if params.use_transducer:
			blank_decoder = self.build_decoder(params)
			vocab_decoder = self.build_decoder(params)		# same structure for both blank and vocab decoder
			joiner = self.build_joiner(params)
		else:
			decoder = None
			joiner = None

		model = ImprovedFactorizedTransducer(
			encoder_embed=encoder_embed,
			encoder=encoder,
			blank_decoder=blank_decoder,
			vocab_decoder=vocab_decoder,
			joiner=joiner,
			args=params,
		)
		
		return model

	
	def ppl_one_batch(
		self,
		y: k2.RaggedTensor,
	):
		"""Compute perplexity for one batch.
			Used in compute_loss for computing one training batch's current ppl
			and compute_validation_loss for test dataset's ppl.
		Args:
			y:	A ragged tensor with 2 axes [utt][label]. It contains labels of each utterance.	[B, S]
		Returns:
			ppl: total perplexity of this batch (not averaged on sentences)
			batch_total_len: total number of words of this batch
			result: dict, which saves the decoded target of vocab_decoder, target of this batch, 
				and the averaged ppl on sentences of the batch.
		"""
		row_splits = y.shape.row_splits(1)
		y_lens = row_splits[1:] - row_splits[:-1]
		blank_id = self.blank_decoder.blank_id
		sos_y = add_sos(y, sos_id=blank_id)			# add blank_id at every sentence's beginning

		lm_target = torch.clone(y.values)			# changed to 1-dim (y could have different lengths), shape shift to [L]
		lm_target = lm_target.to(torch.int64)		
		lm_target[lm_target > blank_id] -= 1		# elements larger than blank_id will be shifted

		# sos_y_padded: [B, S + 1], start with SOS.
		sos_y_padded = sos_y.pad(mode="constant", padding_value=blank_id)
		sos_y_padded = sos_y_padded.to(torch.int64)
		vocab_decoder_out, _ = self.vocab_decoder(sos_y_padded)
		lm_lprobs = self.joiner.vocab_lm_probs_no_softmax(vocab_decoder_out)	# without log_softmax

		ppl = 0
		batch_total_len = 0
		result = dict()
		result["label"] = []
		result["predict"] = []
		criterion = nn.CrossEntropyLoss(reduction="mean")		# average over sentence length

		batch_size = lm_lprobs.shape[0]				# loop over sentences
		for i in range(batch_size):
			ppl += torch.exp(
				criterion(
					lm_lprobs[i, :y_lens[i]], 
					lm_target[batch_total_len: batch_total_len + y_lens[i]]
				)	# averaged over y_lens[i]
			)

			result["predict"].append(
				torch.argmax(lm_lprobs[i, : y_lens[i]], dim=-1)
				.cpu()
				.detach()
				.numpy()
				.tolist()
			)
			result["label"].append(
				lm_target[batch_total_len : batch_total_len + y_lens[i]].cpu().detach().numpy().tolist()
			)
			batch_total_len += y_lens[i]
		
		result['ppl'] = (ppl / batch_size).detach().cpu().item()	# average batch ppl over sentences
		# result['ppl'] = ppl.detach().cpu()			# in metrictracker.write_summary() it will automatically devide num_utterances
		return ppl, batch_total_len, result


	def forward_encoder(
		self, x: torch.Tensor, x_lens: torch.Tensor
	) -> Tuple[torch.Tensor, torch.Tensor]:
		"""Compute encoder outputs.
		Args:
		  x:
			A 3-D tensor of shape (N, T, C).
		  x_lens:
			A 1-D tensor of shape (N,). It contains the number of frames in `x`
			before padding.

		Returns:
		  encoder_out:
			Encoder output, of shape (N, T, C).
		  encoder_out_lens:
			Encoder output lengths, of shape (N,).
		"""
		# logging.info(f"Memory allocated at entry: {torch.cuda.memory_allocated() // 1000000}M")
		x, x_lens = self.encoder_embed(x, x_lens)
		# logging.info(f"Memory allocated after encoder_embed: {torch.cuda.memory_allocated() // 1000000}M")

		src_key_padding_mask = make_pad_mask(x_lens)
		x = x.permute(1, 0, 2)  # (N, T, C) -> (T, N, C)

		encoder_out, encoder_out_lens, layer_features = self.encoder(x, x_lens, src_key_padding_mask)

		encoder_out = encoder_out.permute(1, 0, 2)  # (T, N, C) ->(N, T, C)
		assert torch.all(encoder_out_lens > 0), (x_lens, encoder_out_lens)

		return encoder_out, encoder_out_lens


	def forward_ctc(
		self,
		encoder_out: torch.Tensor,
		encoder_out_lens: torch.Tensor,
		targets: torch.Tensor,
		target_lengths: torch.Tensor,
	) -> torch.Tensor:
		"""Compute CTC loss.
		Args:
		  encoder_out:
			Encoder output, of shape (N, T, C).
		  encoder_out_lens:
			Encoder output lengths, of shape (N,).
		  targets:
			Target Tensor of shape (sum(target_lengths)). The targets are assumed
			to be un-padded and concatenated within 1 dimension.
		"""
		# Compute CTC log-prob
		ctc_output = self.ctc_output(encoder_out)  # (N, T, C)

		ctc_loss = torch.nn.functional.ctc_loss(
			log_probs=ctc_output.permute(1, 0, 2),  # (T, N, C)
			targets=targets,
			input_lengths=encoder_out_lens,
			target_lengths=target_lengths,
			reduction="sum",
		)
		return ctc_loss


	def forward_transducer(
		self,
		encoder_out: torch.Tensor,
		encoder_out_lens: torch.Tensor,
		y: k2.RaggedTensor,
		y_lens: torch.Tensor,
		prune_range: int = 5,
		am_scale: float = 0.0,
		lm_scale: float = 0.0,
		do_prune: bool = False,
	) -> Tuple[torch.Tensor, torch.Tensor]:
		"""Compute Transducer loss.
		Args:
		  encoder_out:
			Encoder output, of shape (N, T, C).
		  encoder_out_lens:
			Encoder output lengths, of shape (N,).
		  y:
			A ragged tensor with 2 axes [utt][label]. It contains labels of each
			utterance.
		  prune_range:
			The prune range for rnnt loss, it means how many symbols(context)
			we are considering for each frame to compute the loss.
		  am_scale:
			The scale to smooth the loss with am (output of encoder network)
			part
		  lm_scale:
			The scale to smooth the loss with lm (output of predictor network)
			part
		"""
		# Now for the decoder, i.e., the prediction network
		blank_id = self.blank_decoder.blank_id
		sos_y = add_sos(y, sos_id=blank_id)

		lm_target = torch.clone(y.values)
		lm_target = lm_target.to(torch.int64)
		lm_target[lm_target>blank_id] -= 1
		# shift because the lm_predictor output does not include the blank_id

		# sos_y_padded: [B, S + 1], start with SOS.
		sos_y_padded = sos_y.pad(mode="constant", padding_value=blank_id)
		sos_y_padded = sos_y_padded.to(torch.int64)

		# decoder_out: [B, S + 1, decoder_dim]
		blank_decoder_out, _ = self.blank_decoder(sos_y_padded)
		vocab_decoder_out, _ = self.vocab_decoder(sos_y_padded)

		lm_lprobs = self.joiner.vocab_lm_probs_no_softmax(vocab_decoder_out)		
		# with no log_softmax, lm_lprobs should not do log_softmax before reshape

		logits, _ = self.joiner(encoder_out, blank_decoder_out, vocab_decoder_out)

		lm_lprobs = [torch.nn.functional.log_softmax(item[:y_len], dim=-1) for item, y_len in zip(lm_lprobs, y_lens)] 
		"""
		sos_y is padded with <sos> in the begining, lm_lprobs will also be S+1 long,
		but we just clip it to y_len, which will be of same length as the original y.
		As for lm_target, it does not do any padding, no blank_id either.
		Therefore, ignore_index is not applicable.
		Meanwhile, nll_loss require input to be log_softmax
		"""
		lm_lprobs = torch.cat(lm_lprobs)

		lm_loss = torch.nn.functional.nll_loss(
			lm_lprobs,
			lm_target,
			reduction="sum"
		)

		# Note: y does not start with SOS
		# y_padded : [B, S]
		y_padded = y.pad(mode="constant", padding_value=0)
		y_padded = y_padded.to(torch.int64)
		boundary = torch.zeros(
			(encoder_out.size(0), 4),
			dtype=torch.int64,
			device=encoder_out.device,
		)
		boundary[:, 2] = y_lens
		boundary[:, 3] = encoder_out_lens

		try:
			if self.use_local_rnnt_loss:
				from rnnt_loss.rnnt_loss import rnnt_loss
			else:
				# from torchaudio.functional import rnnt_loss
				from k2 import rnnt_loss
		except ImportError:
			raise ImportError("Please install a newer torchaudio (version >= 0.10.0)")

		# rnnt_loss = torchaudio.functional.rnnt_loss(
		# 	logits=logits,
		# 	targets=y_padded,
		# 	logit_lengths=x_lens,
		# 	target_lengths=y_lens,
		# 	blank=blank_id,
		# 	reduction="sum",
		# )

		rnnt_loss = rnnt_loss(
			logits=logits,
			symbols=y_padded,
			termination_symbol=blank_id,
			boundary=boundary,
			reduction="sum",
		)	# k2 rnnt loss
	
		return rnnt_loss, lm_loss
		


	def forward(
		self,
		x: torch.Tensor,
		x_lens: torch.Tensor,
		y: k2.RaggedTensor,
		prune_range: int = 5,
		am_scale: float = 0.0,
		lm_scale: float = 0.0,
		do_prune: bool = False,
	) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
		"""
		Args:
		  x:
			A 3-D tensor of shape (N, T, C).
		  x_lens:
			A 1-D tensor of shape (N,). It contains the number of frames in `x`
			before padding.
		  y:
			A ragged tensor with 2 axes [utt][label]. It contains labels of each
			utterance.
		  prune_range:
			The prune range for rnnt loss, it means how many symbols(context)
			we are considering for each frame to compute the loss.
		  am_scale:
			The scale to smooth the loss with am (output of encoder network)
			part
		  lm_scale:
			The scale to smooth the loss with lm (output of predictor network)
			part
		Returns:
		  Return the transducer losses and CTC loss,
		  in form of (simple_loss, pruned_loss, ctc_loss)

		Note:
		   Regarding am_scale & lm_scale, it will make the loss-function one of
		   the form:
			  lm_scale * lm_probs + am_scale * am_probs +
			  (1-lm_scale-am_scale) * combined_probs
		"""
		assert x.ndim == 3, x.shape
		assert x_lens.ndim == 1, x_lens.shape
		assert y.num_axes == 2, y.num_axes

		assert x.size(0) == x_lens.size(0) == y.dim0, (x.shape, x_lens.shape, y.dim0)

		# Compute encoder outputs
		encoder_out, encoder_out_lens = self.forward_encoder(x, x_lens)

		row_splits = y.shape.row_splits(1)
		y_lens = row_splits[1:] - row_splits[:-1]

		rnnt_loss, lm_loss = self.forward_transducer(
			encoder_out=encoder_out,
			encoder_out_lens=encoder_out_lens,
			y=y.to(x.device),
			y_lens=y_lens,
			prune_range=prune_range,
			am_scale=am_scale,
			lm_scale=lm_scale,
			do_prune=do_prune,
		)

		return rnnt_loss, lm_loss


class IFNTDecoder(nn.Module):
	def __init__(
		self,
		vocab_size: int,
		embedding_dim: int,
		blank_id: int,
		num_layers: int,
		hidden_dim: int,
		output_dim: int,
		embedding_dropout: float = 0.0,
		rnn_dropout: float = 0.0,
	):
		"""
		Args:
		  vocab_size:
			Number of tokens of the modeling unit including blank.
		  embedding_dim:
			Dimension of the input embedding.
		  blank_id:
			The ID of the blank symbol.
		  num_layers:
			Number of LSTM layers.
		  hidden_dim:
			Hidden dimension of LSTM layers.
		  output_dim:
			Output dimension of the decoder.
		  embedding_dropout:
			Dropout rate for the embedding layer.
		  rnn_dropout:
			Dropout for LSTM layers.
		"""
		super().__init__()
		self.vocab_size = vocab_size		# same for both blank and vocab decoder.
		self.embedding = nn.Embedding(
			num_embeddings=vocab_size,
			embedding_dim=embedding_dim,
			padding_idx=blank_id,			# should be dictionary without <blk> token, beware
		)
		self.embedding_dropout = nn.Dropout(embedding_dropout)

		self.rnn = nn.LSTM(
			input_size=embedding_dim,
			hidden_size=hidden_dim,
			num_layers=num_layers,
			batch_first=True,
			dropout=rnn_dropout,
		)
		self.blank_id = blank_id
		if output_dim > 0:
		  self.output_linear = nn.Linear(hidden_dim, output_dim)
		else:
		  self.output_linear = lambda x:x


	def forward(
		self,
		y: torch.Tensor,
		states: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
	) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:
		"""
		Args:
		  y:
			A 2-D tensor of shape (N, U) with BOS prepended.
		  states:
			A tuple of two tensors containing the states information of
			LSTM layers in this decoder.
		Returns:
		  Return a tuple containing:

			- rnn_output, a tensor of shape (N, U, C)
			- (h, c), containing the state information for LSTM layers.
			  Both are of shape (num_layers, N, C)
		"""
		embedding_out = self.embedding(y)
		embedding_out = self.embedding_dropout(embedding_out)
		rnn_out, (h, c) = self.rnn(embedding_out, states)
		# out = self.output_linear(rnn_out)

		return rnn_out, (h, c)


class IFNTJoiner(nn.Module):
	def __init__(self, joint_dim: int, encoder_hidden_dim:int, decoder_hidden_dim, vocab_size: int, blank_id: int):
		super().__init__()
		self.joint_dim = joint_dim
		self.encoder_embed_dim = encoder_hidden_dim
		self.decoder_embed_dim = decoder_hidden_dim
		self.blank_id = blank_id
		# add blank symbol in output layer
		self.out_dim = vocab_size
		self.out_blank_dim = 1
		self.out_vocab_dim = self.out_dim - self.out_blank_dim      # V-1

		self.proj_encoder = nn.Linear(self.encoder_embed_dim, self.joint_dim)
		self.laynorm_proj_encoder = LayerNorm(self.joint_dim)
		self.proj_blank_decoder = nn.Linear(self.decoder_embed_dim, self.joint_dim)
		self.laynorm_proj_blank_decoder = LayerNorm(self.joint_dim)

		self.fc_out_blank = nn.Linear(self.joint_dim, self.out_blank_dim)

		self.fc_out_vocab_decoder = nn.Linear(self.joint_dim, self.out_vocab_dim)					# V-1
		# self.dec_out_vocab_decoder = nn.Linear(self.out_vocab_dim, self.decoder_embed_dim)		# decoder_out dim D
		# self.proj_vocab_decoder = nn.Linear(self.decoder_embed_dim, self.joint_dim)				# D -> J
		self.proj_vocab_decoder = nn.Linear(self.out_vocab_dim, self.joint_dim)						# V-1 -> J
		self.laynorm_proj_vocab_decoder = LayerNorm(self.joint_dim)

		self.fc_out_vocab = nn.Linear(self.joint_dim, self.out_vocab_dim)

		nn.init.normal_(self.proj_encoder.weight, mean=0, std=self.joint_dim**-0.5)
		nn.init.normal_(self.proj_blank_decoder.weight, mean=0, std=self.joint_dim**-0.5)
		nn.init.normal_(self.fc_out_blank.weight, mean=0, std=self.joint_dim**-0.5)

		nn.init.normal_(self.fc_out_vocab_decoder.weight, mean=0, std=self.joint_dim**-0.5)
		# nn.init.normal_(self.dec_out_vocab_decoder.weight, mean=0, std=self.joint_dim**-0.5)
		nn.init.normal_(self.proj_vocab_decoder.weight, mean=0, std=self.joint_dim**-0.5)
		nn.init.normal_(self.fc_out_vocab.weight, mean=0, std=self.joint_dim**-0.5)

	# encoder_out: B x T x C
	# decoder_out: B X U x C

	def encoder_proj(self, encoder_out):
		"""_summary_

		Args:
			Output from the encoder. Its shape is (N, T, s_range, C).

		Returns:
			project of the encoder
		"""
		encoder_out = self.encoder_norm(self.encoder_fc(encoder_out))
		return encoder_out
	
	def blank_decoder_proj(self, blank_decoder_out):
		blank_decoder_out = self.blank_decoder_norm(self.blank_decoder_fc(blank_decoder_out))
		return blank_decoder_out

	def vocab_decoder_proj(self, vocab_decoder_out):
		# vocab_decoder_out = self.laynorm_proj_vocab_decoder(vocab_decoder_out)
		vocab_decoder_out = self.vocab_decoder_lm_prob_fc(vocab_decoder_out)
		lm_probs =  torch.nn.functional.log_softmax(vocab_decoder_out, dim=-1)
		vocab_decoder_out = self.vocab_decoder_fc1(lm_probs)         # [V, hidden_states]
		vocab_decoder_out = torch.sigmoid(vocab_decoder_out)
		vocab_decoder_out = self.vocab_decoder_norm(self.vocab_decoder_fc2(vocab_decoder_out))
		return lm_probs, vocab_decoder_out

	def vocab_lm_probs_no_softmax(self, vocab_decoder_out):
		"""
		Args:
			vocab_decoder_out:
				Output from the vocab_decoder. Its shape is (N, U, C).
		Returns:
			output from fc_out_decoder_vocab, without doing log_softmax
			this will be used for lm_loss computation, as padded parts will be excluded.
			return tensor shape: (N, U, C)
		"""
		return self.fc_out_vocab_decoder(vocab_decoder_out)


	def forward(self, encoder_out, blank_decoder_out, vocab_decoder_out, lm_probs=None, project_input=True):
		"""
		Args:
		  encoder_out:
			Output from the encoder. Its shape is (N, T, s_range, C).
		  decoder_out:
			Output from the decoder. Its shape is (N, T, s_range, C).
		   project_input:
			If true, apply input projections encoder_proj and decoder_proj.
			If this is false, it is the user's responsibility to do this
			manually.
		Returns:
		  Return a tensor of shape (N, T, s_range, C).
		"""
		# if project_input:       # False in model.forward()
		#     encoder_out = self.encoder_proj(encoder_out)
		#     blank_decoder_out = self.blank_decoder_proj(blank_decoder_out)
		#     vocab_decoder_out, lm_probs = self.vocab_decoder_proj(vocab_decoder_out)

		# blank_prob = nn.functional.relu(encoder_out + blank_decoder_out)
		# blank_prob = self.blank_out_fc(blank_prob)

		# vocab_out = nn.functional.relu(encoder_out + vocab_decoder_out)
		# vocab_out = self.vocab_out_fc(vocab_out)

		# vocab_out = vocab_out + lm_probs
		# out = torch.cat((vocab_out[:,:,:,:self.blank_id], blank_prob, vocab_out[:,:,:,self.blank_id:]), dim=-1)
		# return out, lm_probs

		encoder_out = self.laynorm_proj_encoder(self.proj_encoder(encoder_out))
		blank_decoder_out = self.laynorm_proj_blank_decoder(self.proj_blank_decoder(blank_decoder_out))
		out_blank = nn.functional.relu(encoder_out.unsqueeze(2) + blank_decoder_out.unsqueeze(1))
		out_blank = self.fc_out_blank(out_blank)

		lm_lprobs = torch.nn.functional.log_softmax(self.fc_out_vocab_decoder(vocab_decoder_out), dim=-1)		# (B, U, V-1)
		# vocab_decoder_out = torch.nn.functional.sigmoid(self.dec_out_vocab_decoder(lm_lprobs))
		# vocab_decoder_out = self.laynorm_proj_vocab_decoder(self.proj_vocab_decoder(vocab_decoder_out))
		vocab_decoder_out = torch.sigmoid(self.proj_vocab_decoder(lm_lprobs))
		vocab_decoder_out = self.laynorm_proj_vocab_decoder(vocab_decoder_out)

		out_vocab = nn.functional.relu(encoder_out.unsqueeze(2) + vocab_decoder_out.unsqueeze(1))
		out_vocab = self.fc_out_vocab(out_vocab)			# (B, T, U, V-1)
		out_vocab = out_vocab + lm_lprobs.unsqueeze(1)
		
		out = torch.cat((out_vocab[:,:,:,:self.blank_id], out_blank, out_vocab[:,:,:,self.blank_id:]), dim=-1)
		return out, lm_lprobs
